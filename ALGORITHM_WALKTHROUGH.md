# OpenRoadMap - Algorithm & Logic Verification Guide

This guide is designed for instructors and evaluators to quickly verify the core algorithms, logic, and performance of the OpenRoadMap project without getting bogged down in boilerplate setup.

## ðŸ§  Core Logic Overview

The project uses a **Retrieval-Augmented Generation (RAG)** approach with **Query Expansion** to generate personalized learning paths.

### 1. The "Brain" (Vector Search)
*   **File**: `src/agents/resource_agent.py`
*   **Algorithm**: Dense Semantic Search using Cosine Similarity.
*   **Model**: `BAAI/bge-small-en-v1.5` (384 dimensions).
*   **Logic**:
    1.  User query is embedded into a vector.
    2.  System searches Qdrant (Vector DB) for nearest neighbors.
    3.  **Fallback**: If local scores are low (< 0.4), it falls back to DuckDuckGo Web Search.

### 2. The "Smarts" (Query Expansion)
*   **File**: `src/agents/resource_agent.py` (Method: `expand_query`)
*   **Logic**:
    1.  User query (e.g., "React") is sent to GPT-3.5.
    2.  LLM generates 3 variations (e.g., "React.js tutorial", "React hooks course", "Best React resources").
    3.  System searches for **ALL** variations and aggregates results (Reciprocal Rank Fusion logic).
    4.  **Impact**: Increases Recall by finding resources that don't match the exact keyword but match the *intent*.

### 3. The "Judge" (Graded Evaluation)
*   **File**: `src/agents/eval_agent.py`
*   **Metric**: **NDCG@5** (Normalized Discounted Cumulative Gain).
*   **Logic**:
    *   Instead of Binary Relevance (Right/Wrong), we use **Graded Relevance** (0-3 scale).
    *   3 = Perfect Match, 2 = Relevant, 1 = Tangential, 0 = Irrelevant.
    *   This rewards the model for finding "Good" resources even if they aren't the "Best".

---

## ðŸ” Code Inspection Points

To verify the logic implementation, check these key files:

| Component | File Path | Key Function/Class |
| :--- | :--- | :--- |
| **Ingestion** | `scripts/ingestion/vectorize_corpus.py` | `vectorize_corpus()` (See embedding generation) |
| **Search** | `src/agents/resource_agent.py` | `find_resources()` (See expansion & fallback logic) |
| **Evaluation** | `src/agents/eval_agent.py` | `evaluate_resources()` (See NDCG calculation) |
| **Synthetic Data** | `scripts/evaluation/generate_synthetic_data.py` | `generate_synthetic_data()` (See GPT-4 generation) |

---

## ðŸ§ª Verification Script (The "Proof")

To verify that the system generates high-quality roadmaps with relevant resources, use the **Roadmap Generator Script**.

### How to Run
```bash
# Generate a roadmap for "Machine Learning"
python scripts/generate_roadmap.py "Machine Learning"
```

### Expected Output
The script will generate a JSON file (`roadmap_output.json`) containing:
1.  **Structure**: A logical DAG of learning topics (generated by GPT-4).
2.  **Resources**: For each topic, a list of curated resources (retrieved from Qdrant/Web).

You can inspect `roadmap_output.json` to verify:
*   **Relevance**: Are the resources actually about the topic?
*   **Diversity**: Do they come from your ingested corpus?
*   **Expansion**: Did it find resources that match the *intent* even if keywords differ?

---

## ðŸ› ï¸ Quick-Start for Logic Testing

If you want to test the logic interactively without the frontend:

1.  **Start the Backend**:
    ```bash
    uvicorn src.main:app --reload
    ```
2.  **Open Swagger UI**: `http://localhost:8000/docs`
3.  **Test Endpoint**: `POST /api/generate-roadmap`
    *   **Payload**: `{"skill": "Machine Learning", "current_knowledge": "Beginner", "goal": "Job Ready"}`
    *   **Verify**: Check the JSON response. Look for `resources` arrays in the nodes. These are populated by the `ResourceAgent`.
